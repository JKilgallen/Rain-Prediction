{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall Prediction - Training a Binary Classifier\r\n",
    "## Exploratory Data Analysis\r\n",
    "### Feature Engineering and Scaling\r\n",
    "As usual we need to load in the libraries, and data, we will be using. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "pd.options.mode.chained_assignment = None\r\n",
    "\r\n",
    "rainfall = pd.read_csv(\"weatherAUS_extracted_dates.csv\")\r\n",
    "rainfall = rainfall[rainfall['RainTomorrow'].notna()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step in preparing our data for use in training and testing a model is to separate out the target variable. We can do this as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = rainfall.drop(columns=['RainTomorrow'])\r\n",
    "y = rainfall['RainTomorrow']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now split up the data into training and testing datasets as follows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\r\n",
    "\r\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to handle the missing values we can impute them into the datasets. What this means is we assign them a value by inferring one from our data. For the numerical variables we'll be using the median since the data contains many outliers. And for the categorical variables we'll be using the mode. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "numerical_vars = X.select_dtypes(include=['int64', 'float64']).columns\r\n",
    "categorical_vars = X.select_dtypes(include=['object']).columns\r\n",
    "\r\n",
    "for X in [X_train, X_test]:\r\n",
    "    for cat_var in categorical_vars:\r\n",
    "        var_mode = X_train[cat_var].mode()[0]\r\n",
    "        X[cat_var].fillna(var_mode, inplace=True);\r\n",
    "    for num_var in numerical_vars:\r\n",
    "        var_median = X_train[num_var].median()\r\n",
    "        X[num_var].fillna(var_median, inplace=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also engineer some of our outliers by capping them at the bounds of the interquartile range. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "for X in [X_train, X_test]:\r\n",
    "    for num_var in numerical_vars:\r\n",
    "        ub = X_train[num_var].quantile(0.75)\r\n",
    "        X.loc[X[num_var] > ub, num_var] = ub\r\n",
    "\r\n",
    "        lb = X_train[num_var].quantile(0.25)\r\n",
    "        X.loc[X[num_var] < lb, num_var] = lb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our second to last step is to use sklearn's `DictVectorizer` to encode our categorical variables so that they can be more easily used in the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction import DictVectorizer\r\n",
    "\r\n",
    "dv = DictVectorizer(sparse=False)\r\n",
    "train_dict = X_train.to_dict(orient='records')\r\n",
    "X_train = dv.fit_transform(train_dict)\r\n",
    "X_test = dv.transform(X_test.to_dict(orient='records'))\r\n",
    "np.save(\"features.npy\", dv.get_feature_names())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lastly, we need to scale all of the variables so that no variable biases the model. We do this as shown below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\r\n",
    "\r\n",
    "scaler = MinMaxScaler()\r\n",
    "X_train = scaler.fit_transform(X_train)\r\n",
    "X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally (this time for real), we can save the dataframes we made so we can use them for training and testing the model in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.savetxt(\"X_train.csv\", X_train, delimiter=',')\r\n",
    "np.savetxt(\"X_test.csv\", X_test, delimiter=',')\r\n",
    "y_train.to_csv(\"y_train.csv\")\r\n",
    "y_test.to_csv(\"y_test.csv\")"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c88a9655f5705c7139cba7d6ba9243020aea30b4a9a55e3d73431b6a757f00f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}