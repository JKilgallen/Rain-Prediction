{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Rainfall Prediction - Evaluating a Binary Classifier\r\n",
    "## Accuracy Score and Confusion Matrix\r\n",
    "As always we need to load in the libraries, and data, we will be using. In this case we can load in the testing and training datasets created in previous notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import numpy as np\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "\r\n",
    "X_train = pd.read_csv(\"X_train.csv\", header=None)\r\n",
    "X_test = pd.read_csv(\"X_test.csv\", header=None)\r\n",
    "y_train = pd.read_csv(\"y_train.csv\")\r\n",
    "y_test = pd.read_csv(\"y_test.csv\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, as in the last notebook, we can train a logistic regression model on our training data, test it using our test data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8328000281303843"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\r\n",
    "from sklearn.metrics import accuracy_score\r\n",
    "\r\n",
    "model = LogisticRegression(solver='liblinear')\r\n",
    "fit = model.fit(X_train, y_train['RainTomorrow'].values.ravel())\r\n",
    "y_pred = model.predict(X_test)\r\n",
    "\r\n",
    "accuracy_score(y_test['RainTomorrow'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now compare this with the performance of the model on the training data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8333069606343513"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train_pred = model.predict(X_train)\r\n",
    "accuracy_score(y_train['RainTomorrow'], y_train_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the difference in the accuracy is minimal i.e. $0.1\\%$ difference. However, since our data isn't balanced i.e. it rains far fewer days than it doesn't, it's worth exploring the performance of our model versus a static prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.22128063574668588"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rain_model = np.repeat(1.0,len(y_test['RainTomorrow']))\r\n",
    "accuracy_score(y_test['RainTomorrow'], rain_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7787193642533141"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dry_model = np.repeat(0.0,len(y_test['RainTomorrow']))\r\n",
    "accuracy_score(y_test['RainTomorrow'], dry_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see from the output above that our model performs better than either of these static models, but only by a margin of about $5\\%$. To get a better understanding of how our model is performing we can use a confusion matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20760,  1386],\n",
       "       [ 3369,  2924]], dtype=int64)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\r\n",
    "\r\n",
    "confusion_matrix(y_test['RainTomorrow'], y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also use `classification_report` to generate several metrics to evaluate model performance, like precision, recall, and $f_{1}$ score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.94      0.90     22146\n",
      "         1.0       0.68      0.46      0.55      6293\n",
      "\n",
      "    accuracy                           0.83     28439\n",
      "   macro avg       0.77      0.70      0.72     28439\n",
      "weighted avg       0.82      0.83      0.82     28439\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(classification_report(y_test['RainTomorrow'], y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To reduce our type II error (also known as the false negative rate), we can adjust the threshold probability for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[17624,  4522],\n",
       "       [ 1570,  4723]], dtype=int64)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_pred = [1 if x > 0.25 else 0 for x in model.predict_proba(X_test)[:,1]]\r\n",
    "confusion_matrix(y_test['RainTomorrow'], new_pred)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "2c88a9655f5705c7139cba7d6ba9243020aea30b4a9a55e3d73431b6a757f00f"
  },
  "kernelspec": {
   "display_name": "Python 3.9.5 64-bit (windows store)",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}